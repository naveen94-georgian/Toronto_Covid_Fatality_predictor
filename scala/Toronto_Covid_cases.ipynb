{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ignored-converter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://cluster-f04b-m:8088/proxy/application_1618597944917_0002\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = yarn, app id = application_1618597944917_0002)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.sql.types.{IntegerType, DoubleType}\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.types.{IntegerType, DoubleType}\n",
    "import org.apache.spark.sql.functions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "radical-communist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class TrainClassifier\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrainClassifier {\n",
    "    def getActiveCases(dataset: DataFrame): DataFrame = {\n",
    "        dataset.filter($\"Outcome\" === \"ACTIVE\")\n",
    "    }\n",
    "    def getPastCases(dataset: DataFrame): DataFrame = {\n",
    "        dataset.filter($\"Outcome\" =!= \"ACTIVE\")\n",
    "    }\n",
    "\n",
    "    def cleanDataset(dataset: DataFrame): DataFrame = {\n",
    "        //Remove Null values\n",
    "        val data_clean = dataset.na.drop(Seq(\"Age Group\"))\n",
    "\n",
    "        //Cast dataset\n",
    "        val df_cast = data_clean.withColumn(\"Episode Date\", to_date($\"Episode Date\", \"yyyy-MM-dd\"))\n",
    "                    .withColumn(\"Reported Date\", to_date($\"Reported Date\", \"yyyy-MM-dd\"))\n",
    "                    .withColumn(\"_id\",  $\"_id\".cast(\"int\"))\n",
    "                    .withColumn(\"Assigned_ID\",  $\"Assigned_ID\".cast(\"int\"))\n",
    "\n",
    "        //Generate Days Since Episode column from Reported Date and Episode Date\n",
    "        val df_clean = df_cast.withColumn(\"Days Since Episode\", datediff($\"Reported Date\", $\"Episode Date\"))\n",
    "                    .where($\"Days Since Episode\" >= 0)\n",
    "\n",
    "        //Select features to train the model\n",
    "        val df_select = df_clean.select(\"Outbreak Associated\", \"Age Group\", \"Source of Infection\", \"Classification\", \"Client Gender\", \"Outcome\", \"Days Since Episode\",\n",
    "                          \"Currently Hospitalized\", \"Currently in ICU\", \"Currently Intubated\", \"Ever Hospitalized\", \"Ever in ICU\", \"Ever Intubated\")\n",
    "        df_select\n",
    "    }\n",
    "\n",
    "    def featureEngg(dataset: DataFrame): DataFrame = {\n",
    "        val indexer = new StringIndexer()\n",
    "      .setInputCols(Array(\"Outbreak Associated\", \"Age Group\", \"Source of Infection\", \"Classification\", \"Client Gender\", \"Outcome\",\n",
    "                          \"Currently Hospitalized\", \"Currently in ICU\", \"Currently Intubated\", \"Ever Hospitalized\", \"Ever in ICU\", \"Ever Intubated\"))\n",
    "      .setOutputCols(Array(\"outbreakAssociatedIdx\", \"ageGroupIdx\", \"sourceOfInfectionIdx\", \"classificationIdx\", \"clientGenderIdx\",\n",
    "                           \"outcomeIdx\", \"currentlyHospitalizedIdx\", \"currentlyInICUIdx\", \"currentlyIntubatedIdx\", \"everHospitalizedIdx\", \"everInICUIdx\", \"everIntubatedIdx\"))\n",
    "\n",
    "        val df = indexer.fit(dataset).transform(dataset)\n",
    "\n",
    "        val select_df = df.select(\"outbreakAssociatedIdx\", \"ageGroupIdx\", \"sourceOfInfectionIdx\", \"classificationIdx\", \"clientGenderIdx\", \"Days Since Episode\",\n",
    "                           \"outcomeIdx\", \"currentlyHospitalizedIdx\", \"currentlyInICUIdx\", \"currentlyIntubatedIdx\", \"everHospitalizedIdx\", \"everInICUIdx\", \"everIntubatedIdx\")\n",
    "        select_df\n",
    "    }\n",
    "\n",
    "    def trainWithRandomForest(dataset: DataFrame): CrossValidatorModel = {\n",
    "\n",
    "        //Split dataset into Train and Test datasets\n",
    "        val Array(train_df, test_df) = dataset.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "        //VectorAssembler to merge multiple features into single feature\n",
    "        val assembler = new VectorAssembler()\n",
    "         .setInputCols(Array(\"outbreakAssociatedIdx\", \"ageGroupIdx\", \"sourceOfInfectionIdx\", \"classificationIdx\", \"clientGenderIdx\", \"Days Since Episode\",\n",
    "                            \"currentlyHospitalizedIdx\", \"currentlyInICUIdx\", \"currentlyIntubatedIdx\", \"everHospitalizedIdx\", \"everInICUIdx\", \"everIntubatedIdx\"))\n",
    "         .setOutputCol(\"assembled-features\")\n",
    "\n",
    "        //Train a classification model using RandomForestClassifier\n",
    "        val rf = new RandomForestClassifier()\n",
    "         .setFeaturesCol(\"assembled-features\")\n",
    "         .setLabelCol(\"outcomeIdx\")\n",
    "\n",
    "        //pipeline for assembler and classifier\n",
    "        val pipeline = new Pipeline()\n",
    "          .setStages(Array(assembler, rf))\n",
    "\n",
    "        //BinaryClassificationEvaluator to find the accuracy of the trained model\n",
    "        val evaluator = new BinaryClassificationEvaluator()\n",
    "          .setLabelCol(\"outcomeIdx\")\n",
    "\n",
    "        //ParamGridBuilder to build different possible combination of parameters\n",
    "        val paramGrid = new ParamGridBuilder()  \n",
    "          .addGrid(rf.maxDepth, Array(3, 5))\n",
    "          .addGrid(rf.impurity, Array(\"entropy\",\"gini\")).build()\n",
    "\n",
    "        //K-Fold cross validation\n",
    "        val cross_validator = new CrossValidator()\n",
    "          .setEstimator(pipeline)\n",
    "          .setEvaluator(evaluator)\n",
    "          .setEstimatorParamMaps(paramGrid)\n",
    "          .setNumFolds(3)\n",
    "\n",
    "        //Fit training data to the classification model\n",
    "        val cvModel = cross_validator.fit(train_df)\n",
    "\n",
    "        //Find predictions with test dataset\n",
    "        val predictions = cvModel.transform(test_df)\n",
    "\n",
    "        //Find the accuracy of predicated values\n",
    "        val accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "        println(\"accuracy = \" + accuracy)\n",
    "\n",
    "        cvModel\n",
    "    }\n",
    "    \n",
    "    def getPredictions(dataset: DataFrame) : DataFrame = {\n",
    "        val active_df = getActiveCases(dataset)\n",
    "        val data_df = getPastCases(dataset)\n",
    "        \n",
    "        val clean_df = cleanDataset(data_df)\n",
    "        val df_fengg = featureEngg(clean_df)\n",
    "        val trainedModel = trainWithRandomForest(df_fengg)\n",
    "        \n",
    "        val active_df_clean = cleanDataset(active_df)\n",
    "        val active_fengg_df = featureEngg(active_df_clean)\n",
    "\n",
    "        val active_df_prediction = trainedModel.transform(active_fengg_df)\n",
    "\n",
    "        active_df_prediction\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convenient-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9704729382780196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [_id: string, Assigned_ID: string ... 16 more fields]\n",
       "active_df_prediction: org.apache.spark.sql.DataFrame = [outbreakAssociatedIdx: double, ageGroupIdx: double ... 15 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read\n",
    ".format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(\"hdfs://10.128.0.9:8020/BigData/COVID19_cases.csv\")\n",
    "\n",
    "val active_df_prediction = new TrainClassifier().getPredictions(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smoking-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Predicted Resolved cases: 7075\n",
      "The Predicted Fatal cases: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fatal_count: Long = 11\n",
       "resolved_count: Long = 7075\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fatal_count = active_df_prediction.select($\"prediction\").where($\"prediction\" === 1.0).count()\n",
    "val resolved_count = active_df_prediction.select($\"prediction\").where($\"prediction\" === 0.0).count()\n",
    "println(\"The Predicted Resolved cases: \"+ resolved_count)\n",
    "println(\"The Predicted Fatal cases: \"+ fatal_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-siemens",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-beads",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-greece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
